# -*- coding: utf-8 -*-
"""AML_Assignment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IRXLbgeYfSgAGn0RyRlHsuxRthb5lJo_

**<center><b>Report: Movie Review Binary Classification</b></center>**

This paper describes the creation of a Keras neural network for binary categorization of movie reviews from the IMDB movie reviews dataset. The neural network's aim is to identify whether an evaluation is favorable or negative.



<u>**Problem Statment:**</u>

The objective of this project is to use Keras to build a deep neural network to predict whether a review will be positive or negative.



<u>**Dataset:**</u>


The IMDB dataset contains 50,000 highly polarized movie reviews that have been split into two equal groups of 25,000 reviews each for training and testing. There are an equal amount of favorable and negative evaluations in each set. Keras includes this dataset, which contains reviews and the labels that go with them, with 0 indicating a negative review and 1 reflecting a good review. The evaluations take the shape of a series of words that have been pre-processed into a series of numbers, each integer signifying a different word from the dictionary.


<u>**Methodology:**</u>

To create the neural network, we used Python's Keras framework. First, the dataset was split into training and test groups. The data was then loaded, with only the 10,000 most commonly appearing terms retained. The evaluations were then decoded to their original text using a lexicon mapping from word to integer value.



A sequential model was then developed, consisting of an embedding layer that takes the integer-encoded vocabulary and translates each word to a feature vector of a given size. This was followed by a dense layer with 16 hidden units and the ReLU activation function, which was then linked to an output layer with a sigmoid activation function to generate a chance between 0 and 1. The model was built with binary cross-entropy as the loss function, the RMSprop optimizer, and accuracy as the assessment measure.

Loading the IMDB dataset
"""

# importing libraries

import matplotlib
import matplotlib.pyplot as plt
import numpy as np
from keras import models
from keras import layers

"""Dataset is divided into train data and test data

Data is loaded and 10,000 most frequently occuring words are kept
"""

from tensorflow.keras.datasets import imdb
(train_data, train_labels), (test_data, test_labels) = imdb.load_data(
    num_words=10000)

"""Printing training data first review


"""

train_data[0]

# Check the first label

train_labels[0]

# 10000 frequent words are taken into account and  word index should not be exceeded to the frequent word count.

# Finding the max of all the max indexes
max([max(sequence) for sequence in train_data])

"""Decoding movie reviews back to text


"""

#Loading the mappings from word to integer index and reverse the word index to integer  and then decode the review by mapping integer to word

# step 1: Load the mappings of the dictionary from the word to integer index
word_index = imdb.get_word_index()

# step 2: reverse word index to integer mapping 
reverse_word_index = dict(
    [(value, key) for (key, value) in word_index.items()])

# step 3: Decode the review, mapping integer to words
decoded_review = " ".join(
    [reverse_word_index.get(i - 3, "?") for i in train_data[0]])
decoded_review

len(reverse_word_index)

"""**<u>Preparing the data:</u>**


* Our deep convolutional neural network cannot be provided a list of numbers. We'll have to transform them into tensors.
* To prepare our data, we'll use One-hot Encoding to transform our lists into vectors of 0s and 1s. Each of our sequences would be blown up into 10,000-dimensional vectors with 1 at all positions connecting all numbers in the sequence. 
* This vector will contain the element 0 for all values that are not in integer order.Simply put, each evaluation will be symbolized by a 10,000-dimensional vector.Each index correlates to a particular phrase.
* Every index with the value 1 indicates a term in the review that is marked by its integer equivalent. Every number that begins with 0 is a name.


"""

import numpy as np
def vectorize_sequences(sequences, dimension=10000):
    results = np.zeros((len(sequences), dimension))                   
    for i, sequence in enumerate(sequences):
        for j in sequence:
            results[i, j] = 1                                 
    return results

# Vectorizing the training and test data
x_train = vectorize_sequences(train_data)
x_test = vectorize_sequences(test_data)

x_train[0]

x_train.shape

y_train = np.asarray(train_labels).astype("float32")
y_test = np.asarray(test_labels).astype("float32")

"""<u>**Making the model:**</u>

Our initial dataset was made up of vectors that needed to be transformed to encoder labels (0s and 1s). This is one of the most fundamental configurations, and a basic stack of completely linked Dense layers with relu activation works well.

<u>**Hidden Layers:**</u>

* In this network, we'll use concealed levels. As a result, we'll categorize our levels.

* Dense(16,'relu' activation) The parameter given to each Dense layer specifies the number of concealed units of a layer (16).

* The output of a Dense layer with relu activation is produced after a sequence of tensor processes. The following is how this procedure is carried out:

* relu(dot(W, input) Plus b) output W is the weight matrix, and b is the skew (tensor).

* If there are 16 concealed units, the matrix W will have the form (_inputDimension, 16). The dimension of the input vector in this situation is 10,000, and the construction of the Weight matrix is (10000, 16). If this network were depicted as a graph, it would have 16 nodes.

<u>**Architecture of the Model**</u>

We will be utilizing the for our example.

* Two intermediary levels, each with 16 hidden layers, employ the relu activation function, which is used to zero out negative numbers.

* The output layer, which employs sigmoid activation, will be the third layer
"""

from tensorflow import keras
from tensorflow.keras import layers

model = keras.Sequential([
    layers.Dense(16, activation="relu"),
    layers.Dense(16, activation="relu"),
    layers.Dense(1, activation="sigmoid")
])

"""<b>**Compiling the model:**</b>

We will look at the optimizer, loss function, and data during the compilation phase.

The following methods will be used in this case.

* The binary crossentropy loss function is employed in binary categorization.
rmsprop is the algorithm used.
* Accuracy is used to assess success.
* Because keras includes all of the rmsprop, binary crossentropy, and accuracy functions, the above methods can be used to build the model.

model.compile(rmsprop, loss="binary crossentropy", metrics="accuracy"])


"""

model.compile(optimizer="rmsprop",
              loss="binary_crossentropy",
              metrics=["accuracy"])

"""**<b>Setting aside a validation set</b>**

We'll save some of our training data to test the model's accuracy as it advances. A validation set enables businesses to track our model's development through epochs during training on previously unknown data.

* We can fine-tune the model's training settings using validation stages.

* To avoid data overfitting and underfitting, use the fit tool.


"""

x_val = x_train[:10000]
partial_x_train = x_train[10000:]
y_val = y_train[:10000]
partial_y_train = y_train[10000:]

"""**<b>Training the model</b>**

For the first 20 epochs, we'll train our models in 512-sample mini-batches. Our confirmation collection will also be used by the fit technique.

When invoked, the fit method will return a History object. This object has a member history that contains all training process information, including the values of visible or tracked quantities as the epochs advance. This object will be saved so that we can better determine how to fine-tune the training process.


"""

history = model.fit(partial_x_train,
                    partial_y_train,
                    epochs=20,
                    batch_size=512,
                    validation_data=(x_val, y_val))

"""* By the conclusion of the training, we had achieved a training accuracy of 99.99% and a confirmation accuracy of 86.80%.

* The network's performance measurements are monitored and saved in the history object.

* The match function returns a history object. This entity has a dictionary with four items as a property.
"""

history_dict = history.history
history_dict.keys()

"""history_dict consists of

* Training loss
* Training Accuracy
* Validation Loss
* Validation Accuracy

We're using Matplotlib to compare the loss and precision of Training and Validation.

Plotting the training and validation loss
"""

# losses 
import matplotlib.pyplot as plt
history_dict = history.history
loss_values = history_dict["loss"]             
val_loss_values = history_dict["val_loss"]
epochs = range(1, len(loss_values) + 1)
plt.plot(epochs, loss_values, "bo", label="Training loss")
plt.plot(epochs, val_loss_values, "b", label="Validation loss")
plt.title("Training and validation loss")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend()
plt.show()

"""Confirmation loss began to increase after the third era. As a result, the remodeling is carried out using the third and fourth epochs.

Plotting the training and validation accuracy
"""

# accuracy 
plt.clf()
acc = history_dict["accuracy"]
val_acc = history_dict["val_accuracy"]                     
plt.plot(epochs, acc, "bo", label="Training acc")
plt.plot(epochs, val_acc, "b", label="Validation acc")
plt.title("Training and validation accuracy")
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.legend()
plt.show()

"""The graphs above show that the lowest validation loss and optimum validation accuracy occur at 3 to 5 epochs. Then we notice two patterns.

* Validation loss rises, while training loss falls.
* The certification accuracy falls while the training accuracy rises.

The preceding consequences imply that while the model improves at classifying the training data, it consistently makes worse forecasts when it meets new and unknown data, indicating overfitting. After the fifth epoch, the algorithm starts to match the training data too closely.

To resolve overfitting, the epoch count is decreased to between 3 and 5 epochs. The epochs may differ based on the machine and the character of randomly allocated weights.

Retraining our model
"""

model = keras.Sequential([
    layers.Dense(16, activation="relu"),
    layers.Dense(16, activation="relu"),
    layers.Dense(1, activation="sigmoid")
])
model.compile(optimizer="rmsprop",
              loss="binary_crossentropy",
              metrics=["accuracy"])

model.fit(x_train, y_train, epochs=3, batch_size=512)     
results = model.evaluate(x_test, y_test)

"""**<b>Result</b>**

The model was trained for 20 epochs and obtained a test accuracy of 88.72%. To illustrate the findings, the model's training and validation loss and accuracy were plotted on graphs. It was discovered that the training and confirmation precision grew concurrently with each epoch, while the loss decreased. Furthermore, the training accuracy was found to be reliably greater than the validation accuracy, indicating that the model is slightly overfitting.
"""

results

"""1. You used two hidden layers. Try using one or three hidden layers, and see how doing so affects validation and test accuracy.


In this report, we will examine two neural networks with various designs to see how the number of hidden layers affects model accuracy.

The first neural network has only one hidden layer with 16 nodes and uses the ReLU activation function. The output layer is composed of only one node and employs the sigmoid activation function. RMSprop is the algorithm used, and binary cross-entropy is the loss function. The group size has been fixed to 512, and the number of epochs to 20.


The second neural network, like the first, has three hidden layers with 16 nodes each and employs the ReLU activation function. The output layer is configured similarly to the first neural network. RMSprop is the algorithm used, and binary cross-entropy is the loss function. The group size has been fixed to 512, and the number of epochs to 20.
"""

# Three hidden layers and one relu activation function model
model_13 = keras.Sequential([
    layers.Dense(16, activation="relu"),
    layers.Dense(16, activation="relu"),
    layers.Dense(16, activation="relu"),
    layers.Dense(1, activation="sigmoid")
])

# one hidden layer and one relu activation function model
model_11 = keras.Sequential([
    layers.Dense(16, activation="relu"),
    layers.Dense(1, activation="sigmoid")
])

#RMSProp and binary cross entropy for both models
model_13.compile(optimizer="rmsprop",
              loss="binary_crossentropy",
              metrics=["accuracy"])

model_11.compile(optimizer="rmsprop",
              loss="binary_crossentropy",
              metrics=["accuracy"])

# Fitting model with 20 epochs and 512 batch size
history_13 = model_13.fit(partial_x_train,
                    partial_y_train,
                    epochs=20,
                    batch_size=512,
                    validation_data=(x_val, y_val))

history_11 = model_11.fit(partial_x_train,
                    partial_y_train,
                    epochs=20,
                    batch_size=512,
                    validation_data=(x_val, y_val))

"""Plotting training and validation loss


"""

historyp_13 = history_13.history
historyp_13.keys()

historyp_11 = history_11.history
historyp_11.keys()

"""Plotting training and testing loss


"""

# Losses
historyp_13 = history_13.history
loss_values3 = historyp_13["loss"]
val_loss_values3 = historyp_13["val_loss"]            
epochs = range(1, len(loss_values) + 1)
plt.plot(epochs, loss_values3, "bo", label="Training loss")
plt.plot(epochs, val_loss_values3, "b", label="Validation loss")
plt.title("Training and validation loss")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend()
plt.show()

"""Here, the minimum validation loss is observed at 5th epoch


"""

#Losses
historyp_11 = history_11.history
loss_values1 = historyp_11["loss"]
val_loss_values1 = historyp_11["val_loss"]           
epochs = range(1, len(loss_values) + 1)
plt.plot(epochs, loss_values1, "bo", label="Training loss")
plt.plot(epochs, val_loss_values1, "b", label="Validation loss")
plt.title("Training and validation loss")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend()
plt.show()

"""Here, the minimum validation loss is observed at 5th epoch

Plotting training and testing accuracy
"""

#Accuracy
plt.clf()
acc3 = historyp_13["accuracy"]
val_acc3 = historyp_13["val_accuracy"]           
plt.plot(epochs, acc3, "bo", label="Training acc")
plt.plot(epochs, val_acc3, "b", label="Validation acc")
plt.title("Training and validation accuracy")
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.legend()
plt.show()

"""Here, the maximum validation accuracy is observed at 5th epoch


"""

#Accuracy
plt.clf()
acc1 = historyp_11["accuracy"]
val_acc1 = historyp_11["val_accuracy"]          
plt.plot(epochs, acc1, "bo", label="Training acc")
plt.plot(epochs, val_acc1, "b", label="Validation acc")
plt.title("Training and validation accuracy")
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.legend()
plt.show()

"""Here, the maximum vallidation accuracy is observed at 5th epoch.

**<b>Results:</b>**

Both models are trained on the same dataset, and their performance is evaluated on the validation set, which is different from the training set. The following table shows the accuracy of both neural networks on the validation and test datasets.

Model	
One	 Validation Set : 0.8868	Accuracy on Test Set : 0.8744

Three	Validation Set : 0.8824	 Accuracy on Test Set : 0.8716

From the table above, we can see that the neural network with one hidden layer performed better than the neural network with three hidden layers. The difference in accuracy is relatively small, but it is consistent on both the validation and test datasets.

2. Try using layers with more hidden units or fewer hidden units: 32 units, 64 units, and so on.
"""

#2 hidden layers and 32 and 64 nodes model
model_2 = keras.Sequential([
    layers.Dense(32, activation="relu"),
    layers.Dense(64, activation="relu"),
    layers.Dense(1, activation="sigmoid")
])

#RMSProp and binary cross entropy 
model_2.compile(optimizer="rmsprop",
              loss="binary_crossentropy",
              metrics=["accuracy"])

# Fitting model with 20 epochs and 512 batch size
history2 = model_2.fit(partial_x_train,
                    partial_y_train,
                    epochs=20,
                    batch_size=512,
                    validation_data=(x_val, y_val))

historyp2 = history2.history
loss_values = historyp2["loss"]
val_loss_values = historyp2["val_loss"]
epochs = range(1, len(loss_values) + 1)
plt.plot(epochs, loss_values, "bo", label="Training loss")
plt.plot(epochs, val_loss_values, "b", label="Validation loss")
plt.title("Training and validation loss")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend()
plt.show()

plt.clf()
acc = historyp2["accuracy"]
val_acc = historyp2["val_accuracy"]
plt.plot(epochs, acc, "bo", label="Training acc")
plt.plot(epochs, val_acc, "b", label="Validation acc")
plt.title("Training and validation accuracy")
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.legend()
plt.show()

"""The minimum validation loss is observed at 3rd epoch and maximum validation accuracy is observed at 3rd and 4th epochs.

3. Try using the mse loss function instead of binary_crossentropy.
"""

model_3 = keras.Sequential([
    layers.Dense(16, activation="relu"),
    layers.Dense(16, activation="relu"),
    layers.Dense(1, activation="sigmoid")
])

#RMSProp and mse loss function 
model_3.compile(optimizer="rmsprop",
              loss="mse",
              metrics=["accuracy"])

# Fitting model with 20 epochs and 512 batch size
history3 = model_3.fit(partial_x_train,
                    partial_y_train,
                    epochs=20,
                    batch_size=512,
                    validation_data=(x_val, y_val))

"""Plotting training and testing loss


"""

#Losses
historyp3 = history3.history
loss_values = historyp3["loss"]
val_loss_values = historyp3["val_loss"]             
epochs = range(1, len(loss_values) + 1)
plt.plot(epochs, loss_values, "bo", label="Training loss")
plt.plot(epochs, val_loss_values, "b", label="Validation loss")
plt.title("Training and validation loss")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend()
plt.show()

"""Here, the minimum validation loss is observed in 3rd epoch.


"""

#Accuracy
plt.clf()
acc = historyp3["accuracy"]
val_acc = historyp3["val_accuracy"]            
plt.plot(epochs, acc, "bo", label="Training acc")
plt.plot(epochs, val_acc, "b", label="Validation acc")
plt.title("Training and validation accuracy")
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.legend()
plt.show()

"""Maximum accuracy is seen in 2nd and 3rd epochs

4. Using the tanh activation
"""

#tanh activation function model
model_4 = keras.Sequential([
    layers.Dense(16, activation="tanh"),
    layers.Dense(16, activation="tanh"),
    layers.Dense(1, activation="sigmoid")
])

#RMSProp and binary cross entropy 
model_4.compile(optimizer="rmsprop",
              loss="mse",
              metrics=["accuracy"])

# Fitting model with 20 epochs and 512 batch size
history4 = model_4.fit(partial_x_train,
                    partial_y_train,
                    epochs=20,
                    batch_size=512,
                    validation_data=(x_val, y_val))

historyp4 = history4.history
loss_values = historyp4["loss"]
val_loss_values = historyp4["val_loss"]
epochs = range(1, len(loss_values) + 1)
plt.plot(epochs, loss_values, "bo", label="Training loss")
plt.plot(epochs, val_loss_values, "b", label="Validation loss")
plt.title("Training and validation loss")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend()
plt.show()

plt.clf()
acc = historyp4["accuracy"]
val_acc = historyp4["val_accuracy"]
plt.plot(epochs, acc, "bo", label="Training acc")
plt.plot(epochs, val_acc, "b", label="Validation acc")
plt.title("Training and validation accuracy")
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.legend()

"""Maximum validation accuracy is seen in 3rd epoch where as Minimum validation loss is also seen in 3rd epoch


"""

results

"""5. Using technique we studied in class"""

model_5 = keras.Sequential([
    layers.Dense(32, activation="relu"),
    layers.Dropout(0.2),
    layers.Dense(64, activation="relu"),
    layers.Dense(1, activation="sigmoid")
])

#RMSProp and binary cross entropy 
model_5.compile(optimizer="rmsprop",
              loss="binary_crossentropy",
              metrics=["accuracy"])

# Fitting model with 20 epochs and 512 batch size
history5 = model_5.fit(partial_x_train,
                    partial_y_train,
                    epochs=20,
                    batch_size=512,
                    validation_data=(x_val, y_val))

#Losses
historyp5 = history5.history
loss_values = historyp5["loss"]
val_loss_values = historyp5["val_loss"]        
epochs = range(1, len(loss_values) + 1)
plt.plot(epochs, loss_values, "bo", label="Training loss")
plt.plot(epochs, val_loss_values, "b", label="Validation loss")
plt.title("Training and validation loss")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend()
plt.show()

#Accuracy
plt.clf()
acc = historyp5["accuracy"]    
val_acc = historyp5["val_accuracy"]            
plt.plot(epochs, acc, "bo", label="Training acc")
plt.plot(epochs, val_acc, "b", label="Validation acc")
plt.title("Training and validation accuracy")
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.legend()
plt.show()

"""Maximum accuracy and minimum validation loss is seen in 3rd epoch 

"""

results